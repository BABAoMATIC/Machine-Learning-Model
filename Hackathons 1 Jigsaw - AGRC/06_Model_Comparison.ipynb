{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Comparison - Jigsaw Agile Community Rules Classification\n",
        "\n",
        "This notebook compares all 5 models and selects the best performing one for the Jigsaw Agile Community Rules Classification hackathon.\n",
        "\n",
        "## Models Compared:\n",
        "1. **Logistic Regression** - TF-IDF (15,000 features, n-grams 1-2)\n",
        "2. **Random Forest** - TF-IDF (20,000 features, n-grams 1-3)\n",
        "3. **XGBoost** - TF-IDF (20,000 features, n-grams 1-3)\n",
        "4. **SVM** - TF-IDF (20,000 features, n-grams 1-3)\n",
        "5. **MLP (PyTorch)** - TF-IDF (20,000 features, n-grams 1-3)\n",
        "\n",
        "## Target: Achieve >92% accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Starting Model Comparison...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data (same preprocessing as individual models)\n",
        "train_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n",
        "\n",
        "print(f\"Train dataset shape: {train_df.shape}\")\n",
        "print(f\"Test dataset shape: {test_df.shape}\")\n",
        "\n",
        "# Data augmentation\n",
        "def augment_training_data(df):\n",
        "    augmented_data = []\n",
        "    for _, row in df.iterrows():\n",
        "        augmented_data.append({\n",
        "            'body': row['body'], 'rule': row['rule'], 'subreddit': row['subreddit'], 'rule_violation': row['rule_violation']\n",
        "        })\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        if pd.notna(row['positive_example_1']):\n",
        "            augmented_data.append({'body': row['positive_example_1'], 'rule': row['rule'], 'subreddit': row['subreddit'], 'rule_violation': 1})\n",
        "        if pd.notna(row['positive_example_2']):\n",
        "            augmented_data.append({'body': row['positive_example_2'], 'rule': row['rule'], 'subreddit': row['subreddit'], 'rule_violation': 1})\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        if pd.notna(row['negative_example_1']):\n",
        "            augmented_data.append({'body': row['negative_example_1'], 'rule': row['rule'], 'subreddit': row['subreddit'], 'rule_violation': 0})\n",
        "        if pd.notna(row['negative_example_2']):\n",
        "            augmented_data.append({'body': row['negative_example_2'], 'rule': row['rule'], 'subreddit': row['subreddit'], 'rule_violation': 0})\n",
        "    \n",
        "    return pd.DataFrame(augmented_data)\n",
        "\n",
        "# Feature engineering\n",
        "def add_features(df):\n",
        "    df = df.copy()\n",
        "    df['has_url'] = df['body'].apply(lambda x: 1 if ('http' in str(x).lower() or 'www' in str(x).lower()) else 0)\n",
        "    df['body_length'] = df['body'].apply(lambda x: len(str(x)))\n",
        "    df['word_count'] = df['body'].apply(lambda x: len(str(x).split()))\n",
        "    df['avg_word_length'] = df['body'].apply(lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0)\n",
        "    df['exclamation_count'] = df['body'].apply(lambda x: str(x).count('!'))\n",
        "    df['question_count'] = df['body'].apply(lambda x: str(x).count('?'))\n",
        "    df['caps_ratio'] = df['body'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / len(str(x)) if len(str(x)) > 0 else 0)\n",
        "    df['digit_count'] = df['body'].apply(lambda x: sum(1 for c in str(x) if c.isdigit()))\n",
        "    return df\n",
        "\n",
        "def combine_text(row):\n",
        "    return f\"{row['body']} [SEP] Rule: {row['rule']} [SEP] Subreddit: {row['subreddit']} [SEP] URL: {row['has_url']} [SEP] Length: {row['body_length']} [SEP] Words: {row['word_count']} [SEP] AvgWordLen: {row['avg_word_length']:.1f} [SEP] Exclamations: {row['exclamation_count']} [SEP] Questions: {row['question_count']} [SEP] CapsRatio: {row['caps_ratio']:.2f} [SEP] Digits: {row['digit_count']}\"\n",
        "\n",
        "# Process data\n",
        "augmented_train = augment_training_data(train_df)\n",
        "augmented_train = add_features(augmented_train)\n",
        "test_df = add_features(test_df)\n",
        "augmented_train['enhanced_text'] = augmented_train.apply(combine_text, axis=1)\n",
        "test_df['enhanced_text'] = test_df.apply(combine_text, axis=1)\n",
        "\n",
        "print(f\"Augmented train data size: {len(augmented_train)}\")\n",
        "print(f\"Class distribution: {augmented_train['rule_violation'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data and create TF-IDF features\n",
        "X = augmented_train['enhanced_text']\n",
        "y = augmented_train['rule_violation']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Enhanced TF-IDF with multiple configurations\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "tfidf_configs = [\n",
        "    {'max_features': 15000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95},\n",
        "    {'max_features': 20000, 'ngram_range': (1, 3), 'min_df': 3, 'max_df': 0.9},\n",
        "    {'max_features': 10000, 'ngram_range': (2, 4), 'min_df': 2, 'max_df': 0.95}\n",
        "]\n",
        "\n",
        "X_train_features = []\n",
        "X_val_features = []\n",
        "X_test_features = []\n",
        "\n",
        "for i, config in enumerate(tfidf_configs):\n",
        "    print(f\"Creating TF-IDF config {i+1}: {config}\")\n",
        "    tfidf = TfidfVectorizer(stop_words='english', **config)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "    X_val_tfidf = tfidf.transform(X_val)\n",
        "    X_test_tfidf = tfidf.transform(test_df['enhanced_text'])\n",
        "    \n",
        "    X_train_features.append(X_train_tfidf)\n",
        "    X_val_features.append(X_val_tfidf)\n",
        "    X_test_features.append(X_test_tfidf)\n",
        "\n",
        "# Combine features\n",
        "X_train_combined = hstack(X_train_features)\n",
        "X_val_combined = hstack(X_val_features)\n",
        "X_test_combined = hstack(X_test_features)\n",
        "\n",
        "print(f\"Combined feature matrix shape: {X_train_combined.shape}\")\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models and compare results\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING ALL MODELS FOR COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "models = {}\n",
        "\n",
        "# 1. Logistic Regression\n",
        "print(\"\\n1. Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=2000, class_weight='balanced', C=0.1, solver='liblinear')\n",
        "lr_model.fit(X_train_combined, y_train)\n",
        "lr_pred = lr_model.predict_proba(X_val_combined)[:, 1]\n",
        "lr_auc = roc_auc_score(y_val, lr_pred)\n",
        "lr_acc = accuracy_score(y_val, (lr_pred > 0.5).astype(int))\n",
        "models['Logistic Regression'] = {'model': lr_model, 'auc': lr_auc, 'accuracy': lr_acc}\n",
        "print(f\"   AUC: {lr_auc:.4f}, Accuracy: {lr_acc:.4f} ({lr_acc*100:.2f}%)\")\n",
        "\n",
        "# 2. Random Forest\n",
        "print(\"\\n2. Training Random Forest...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=1000, max_depth=15, min_samples_split=5, min_samples_leaf=2, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "rf_model.fit(X_train_combined, y_train)\n",
        "rf_pred = rf_model.predict_proba(X_val_combined)[:, 1]\n",
        "rf_auc = roc_auc_score(y_val, rf_pred)\n",
        "rf_acc = accuracy_score(y_val, (rf_pred > 0.5).astype(int))\n",
        "models['Random Forest'] = {'model': rf_model, 'auc': rf_auc, 'accuracy': rf_acc}\n",
        "print(f\"   AUC: {rf_auc:.4f}, Accuracy: {rf_acc:.4f} ({rf_acc*100:.2f}%)\")\n",
        "\n",
        "# 3. XGBoost\n",
        "print(\"\\n3. Training XGBoost...\")\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=8, subsample=0.8, colsample_bytree=0.8, random_state=42, eval_metric='auc')\n",
        "xgb_model.fit(X_train_combined, y_train, eval_set=[(X_val_combined, y_val)], verbose=False)\n",
        "xgb_pred = xgb_model.predict_proba(X_val_combined)[:, 1]\n",
        "xgb_auc = roc_auc_score(y_val, xgb_pred)\n",
        "xgb_acc = accuracy_score(y_val, (xgb_pred > 0.5).astype(int))\n",
        "models['XGBoost'] = {'model': xgb_model, 'auc': xgb_auc, 'accuracy': xgb_acc}\n",
        "print(f\"   AUC: {xgb_auc:.4f}, Accuracy: {xgb_acc:.4f} ({xgb_acc*100:.2f}%)\")\n",
        "\n",
        "# 4. SVM\n",
        "print(\"\\n4. Training SVM...\")\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42, class_weight='balanced', C=1.0)\n",
        "svm_model.fit(X_train_combined, y_train)\n",
        "svm_pred = svm_model.predict_proba(X_val_combined)[:, 1]\n",
        "svm_auc = roc_auc_score(y_val, svm_pred)\n",
        "svm_acc = accuracy_score(y_val, (svm_pred > 0.5).astype(int))\n",
        "models['SVM'] = {'model': svm_model, 'auc': svm_auc, 'accuracy': svm_acc}\n",
        "print(f\"   AUC: {svm_auc:.4f}, Accuracy: {svm_acc:.4f} ({svm_acc*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. MLP (PyTorch) - Simplified version\n",
        "print(\"\\n5. Training MLP (PyTorch)...\")\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, dropout=0.3):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Convert to dense for PyTorch\n",
        "X_train_dense = X_train_combined.toarray()\n",
        "X_val_dense = X_val_combined.toarray()\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train_dense)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
        "X_val_tensor = torch.FloatTensor(X_val_dense)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "mlp_model = MLPClassifier(X_train_dense.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "mlp_model.train()\n",
        "for epoch in tqdm(range(10), desc=\"Training MLP\"):\n",
        "    epoch_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp_model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "# Evaluate MLP\n",
        "mlp_model.eval()\n",
        "with torch.no_grad():\n",
        "    mlp_pred = mlp_model(X_val_tensor).numpy().flatten()\n",
        "\n",
        "mlp_auc = roc_auc_score(y_val, mlp_pred)\n",
        "mlp_acc = accuracy_score(y_val, (mlp_pred > 0.5).astype(int))\n",
        "models['MLP'] = {'model': mlp_model, 'auc': mlp_auc, 'accuracy': mlp_acc}\n",
        "print(f\"   AUC: {mlp_auc:.4f}, Accuracy: {mlp_acc:.4f} ({mlp_acc*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Comparison Results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Sort models by accuracy\n",
        "sorted_models = sorted(models.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
        "\n",
        "print(f\"{'Rank':<4} {'Model':<20} {'AUC':<8} {'Accuracy':<10} {'Target (>92%)':<12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for rank, (model_name, results) in enumerate(sorted_models, 1):\n",
        "    auc = results['auc']\n",
        "    acc = results['accuracy']\n",
        "    target_met = \"YES\" if acc > 0.92 else \"NO\"\n",
        "    status = \"🏆 BEST\" if rank == 1 else \"  \"\n",
        "    print(f\"{rank:<4} {status} {model_name:<15} {auc:<8.4f} {acc:<10.4f} {target_met:<12}\")\n",
        "\n",
        "# Find best model\n",
        "best_model_name = sorted_models[0][0]\n",
        "best_model = sorted_models[0][1]['model']\n",
        "best_auc = sorted_models[0][1]['auc']\n",
        "best_accuracy = sorted_models[0][1]['accuracy']\n",
        "\n",
        "print(f\"\\n🏆 BEST MODEL: {best_model_name}\")\n",
        "print(f\"📊 Best AUC: {best_auc:.4f}\")\n",
        "print(f\"📈 Best Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "print(f\"🎯 Target Achieved (>92%): {'YES' if best_accuracy > 0.92 else 'NO'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate final predictions with best model\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"GENERATING FINAL PREDICTIONS WITH BEST MODEL: {best_model_name}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate test predictions\n",
        "if best_model_name == 'MLP':\n",
        "    best_model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_predictions = best_model(torch.FloatTensor(X_test_combined.toarray())).numpy().flatten()\n",
        "else:\n",
        "    test_predictions = best_model.predict_proba(X_test_combined)[:, 1]\n",
        "\n",
        "print(f\"Test predictions generated: {len(test_predictions)}\")\n",
        "print(f\"Prediction range: [{min(test_predictions):.4f}, {max(test_predictions):.4f}]\")\n",
        "print(f\"Mean prediction: {np.mean(test_predictions):.4f}\")\n",
        "\n",
        "# Create final submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'row_id': test_df['row_id'],\n",
        "    'rule_violation': test_predictions\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission_path = '/kaggle/working/best_model_submission.csv'\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"\\nFinal submission saved to: {submission_path}\")\n",
        "print(f\"Submission shape: {submission_df.shape}\")\n",
        "print(f\"\\nFirst few predictions:\")\n",
        "print(submission_df.head())\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL COMPETITION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset: Jigsaw Agile Community Rules Classification\")\n",
        "print(f\"Models Tested: 5 (Logistic Regression, Random Forest, XGBoost, SVM, MLP)\")\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"Best AUC: {best_auc:.4f}\")\n",
        "print(f\"Best Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "print(f\"Target Achieved (>92%): {'✅ YES' if best_accuracy > 0.92 else '❌ NO'}\")\n",
        "print(f\"Final Submission: {submission_path}\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
