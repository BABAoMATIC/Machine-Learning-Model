{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jigsaw Agile Community Rules Classification\n",
        "\n",
        "This notebook implements multiple machine learning models to achieve the highest AUC score (target ‚â•0.95) for the Jigsaw Agile Community Rules Classification hackathon.\n",
        "\n",
        "## Dataset Information\n",
        "- **Train data**: Contains rule violations with positive/negative examples\n",
        "- **Test data**: Similar structure but without rule_violation labels\n",
        "- **Evaluation metric**: Column-averaged AUC\n",
        "\n",
        "## Models to Test:\n",
        "1. Logistic Regression on TF-IDF\n",
        "2. Random Forest Classifier on TF-IDF\n",
        "3. XGBoost Classifier on TF-IDF\n",
        "4. SVM on TF-IDF\n",
        "5. MLP (PyTorch) on TF-IDF embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset shape: (2029, 9)\n",
            "Test dataset shape: (10, 8)\n",
            "\n",
            "Train dataset columns: ['row_id', 'body', 'rule', 'subreddit', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2', 'rule_violation']\n",
            "\n",
            "Train dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2029 entries, 0 to 2028\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   row_id              2029 non-null   int64 \n",
            " 1   body                2029 non-null   object\n",
            " 2   rule                2029 non-null   object\n",
            " 3   subreddit           2029 non-null   object\n",
            " 4   positive_example_1  2029 non-null   object\n",
            " 5   positive_example_2  2029 non-null   object\n",
            " 6   negative_example_1  2029 non-null   object\n",
            " 7   negative_example_2  2029 non-null   object\n",
            " 8   rule_violation      2029 non-null   int64 \n",
            "dtypes: int64(2), object(7)\n",
            "memory usage: 142.8+ KB\n",
            "None\n",
            "\n",
            "First few rows of train data:\n",
            "   row_id                                               body  \\\n",
            "0       0  Banks don't want you to know this! Click here ...   \n",
            "1       1  SD Stream [ ENG Link 1] (http://www.sportsstre...   \n",
            "2       2  Lol. Try appealing the ban and say you won't d...   \n",
            "3       3  she will come your home open her legs with  an...   \n",
            "4       4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
            "\n",
            "                                                rule      subreddit  \\\n",
            "0  No Advertising: Spam, referral links, unsolici...     Futurology   \n",
            "1  No Advertising: Spam, referral links, unsolici...  soccerstreams   \n",
            "2  No legal advice: Do not offer or request legal...   pcmasterrace   \n",
            "3  No Advertising: Spam, referral links, unsolici...            sex   \n",
            "4  No Advertising: Spam, referral links, unsolici...    hearthstone   \n",
            "\n",
            "                                  positive_example_1  \\\n",
            "0  If you could tell your younger self something ...   \n",
            "1  [I wanna kiss you all over! Stunning!](http://...   \n",
            "2  Don't break up with him or call the cops.  If ...   \n",
            "3  Selling Tyrande codes for 3‚Ç¨ to paypal. PM. \\n...   \n",
            "4   wow!! amazing reminds me of the old days.Well...   \n",
            "\n",
            "                                  positive_example_2  \\\n",
            "0  hunt for lady for jack off in neighbourhood ht...   \n",
            "1  LOLGA.COM is One of the First Professional Onl...   \n",
            "2  It'll be dismissed: https://en.wikipedia.org/w...   \n",
            "3  tight pussy watch for your cock get her at thi...   \n",
            "4  seek for lady for sex in around http://p77.pl/...   \n",
            "\n",
            "                                  negative_example_1  \\\n",
            "0  Watch Golden Globe Awards 2017 Live Online in ...   \n",
            "1  #Rapper \\nüö®Straight Outta Cross Keys SC üö®YouTu...   \n",
            "2  Where is there a site that still works where y...   \n",
            "3  NSFW(obviously) http://spankbang.com/iy3u/vide...   \n",
            "4  must be watch movie https://sites.google.com/s...   \n",
            "\n",
            "                                  negative_example_2  rule_violation  \n",
            "0  DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...               0  \n",
            "1  [15 Amazing Hidden Features Of Google Search Y...               0  \n",
            "2  Because this statement of his is true. It isn'...               1  \n",
            "3  Good News ::Download WhatsApp 2.16.230 APK for...               1  \n",
            "4  We're streaming Pokemon Veitnamese Crystal RIG...               1  \n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Train dataset shape: {train_df.shape}\")\n",
        "print(f\"Test dataset shape: {test_df.shape}\")\n",
        "print(f\"\\nTrain dataset columns: {train_df.columns.tolist()}\")\n",
        "print(f\"\\nTrain dataset info:\")\n",
        "print(train_df.info())\n",
        "print(f\"\\nFirst few rows of train data:\")\n",
        "print(train_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original train data size: 2029\n",
            "Augmented train data size: 10145\n",
            "\n",
            "Class distribution in augmented data:\n",
            "rule_violation\n",
            "1    5089\n",
            "0    5056\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Data augmentation: Add positive and negative examples\n",
        "def augment_training_data(df):\n",
        "    \"\"\"\n",
        "    Augment training data by adding positive_example_1/2 as positive samples (rule_violation=1)\n",
        "    and negative_example_1/2 as negative samples (rule_violation=0)\n",
        "    \"\"\"\n",
        "    augmented_data = []\n",
        "    \n",
        "    # Add original data\n",
        "    for _, row in df.iterrows():\n",
        "        augmented_data.append({\n",
        "            'body': row['body'],\n",
        "            'rule': row['rule'],\n",
        "            'subreddit': row['subreddit'],\n",
        "            'rule_violation': row['rule_violation']\n",
        "        })\n",
        "    \n",
        "    # Add positive examples (rule_violation=1)\n",
        "    for _, row in df.iterrows():\n",
        "        if pd.notna(row['positive_example_1']):\n",
        "            augmented_data.append({\n",
        "                'body': row['positive_example_1'],\n",
        "                'rule': row['rule'],\n",
        "                'subreddit': row['subreddit'],\n",
        "                'rule_violation': 1\n",
        "            })\n",
        "        \n",
        "        if pd.notna(row['positive_example_2']):\n",
        "            augmented_data.append({\n",
        "                'body': row['positive_example_2'],\n",
        "                'rule': row['rule'],\n",
        "                'subreddit': row['subreddit'],\n",
        "                'rule_violation': 1\n",
        "            })\n",
        "    \n",
        "    # Add negative examples (rule_violation=0)\n",
        "    for _, row in df.iterrows():\n",
        "        if pd.notna(row['negative_example_1']):\n",
        "            augmented_data.append({\n",
        "                'body': row['negative_example_1'],\n",
        "                'rule': row['rule'],\n",
        "                'subreddit': row['subreddit'],\n",
        "                'rule_violation': 0\n",
        "            })\n",
        "        \n",
        "        if pd.notna(row['negative_example_2']):\n",
        "            augmented_data.append({\n",
        "                'body': row['negative_example_2'],\n",
        "                'rule': row['rule'],\n",
        "                'subreddit': row['subreddit'],\n",
        "                'rule_violation': 0\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(augmented_data)\n",
        "\n",
        "# Augment the training data\n",
        "augmented_train = augment_training_data(train_df)\n",
        "print(f\"Original train data size: {len(train_df)}\")\n",
        "print(f\"Augmented train data size: {len(augmented_train)}\")\n",
        "print(f\"\\nClass distribution in augmented data:\")\n",
        "print(augmented_train['rule_violation'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features added successfully!\n",
            "\n",
            "Augmented train columns: ['body', 'rule', 'subreddit', 'rule_violation', 'has_url', 'body_length']\n",
            "\n",
            "Sample of new features:\n",
            "                                                body  has_url  body_length\n",
            "0  Banks don't want you to know this! Click here ...        0           59\n",
            "1  SD Stream [ ENG Link 1] (http://www.sportsstre...        1           91\n",
            "2  Lol. Try appealing the ban and say you won't d...        0           57\n",
            "3  she will come your home open her legs with  an...        1           75\n",
            "4  code free tyrande --->>> [Imgur](http://i.imgu...        1          313\n"
          ]
        }
      ],
      "source": [
        "# Feature engineering: Add has_url and body_length features\n",
        "def add_features(df):\n",
        "    \"\"\"\n",
        "    Add 'has_url' and 'body_length' features to the dataframe\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # has_url: 1 if 'http' or 'www' in body, else 0\n",
        "    df['has_url'] = df['body'].apply(lambda x: 1 if ('http' in str(x).lower() or 'www' in str(x).lower()) else 0)\n",
        "    \n",
        "    # body_length: length of body\n",
        "    df['body_length'] = df['body'].apply(lambda x: len(str(x)))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Add features to augmented training data\n",
        "augmented_train = add_features(augmented_train)\n",
        "\n",
        "# Add features to test data\n",
        "test_df = add_features(test_df)\n",
        "\n",
        "print(\"Features added successfully!\")\n",
        "print(f\"\\nAugmented train columns: {augmented_train.columns.tolist()}\")\n",
        "print(f\"\\nSample of new features:\")\n",
        "print(augmented_train[['body', 'has_url', 'body_length']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text combination completed!\n",
            "\n",
            "Sample of combined text:\n",
            "Banks don't want you to know this! Click here to know more! [SEP] Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed. [SEP] Subreddit: Futurology [SEP] Has URL: 0 [SEP] Length: 59...\n"
          ]
        }
      ],
      "source": [
        "# Combine text with [SEP] separators\n",
        "def combine_text(row):\n",
        "    \"\"\"\n",
        "    Combine text as 'body [SEP] Rule: rule [SEP] Subreddit: subreddit [SEP] Has URL: has_url [SEP] Length: body_length'\n",
        "    \"\"\"\n",
        "    combined = f\"{row['body']} [SEP] Rule: {row['rule']} [SEP] Subreddit: {row['subreddit']} [SEP] Has URL: {row['has_url']} [SEP] Length: {row['body_length']}\"\n",
        "    return combined\n",
        "\n",
        "# Apply text combination to augmented training data\n",
        "augmented_train['combined_text'] = augmented_train.apply(combine_text, axis=1)\n",
        "\n",
        "# Apply text combination to test data\n",
        "test_df['combined_text'] = test_df.apply(combine_text, axis=1)\n",
        "\n",
        "print(\"Text combination completed!\")\n",
        "print(f\"\\nSample of combined text:\")\n",
        "print(augmented_train['combined_text'].iloc[0][:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 8116\n",
            "Validation set size: 2029\n",
            "\n",
            "Training set class distribution:\n",
            "rule_violation\n",
            "1    4071\n",
            "0    4045\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Validation set class distribution:\n",
            "rule_violation\n",
            "1    1018\n",
            "0    1011\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Split augmented train into 80% train and 20% validation, stratified by rule_violation\n",
        "X = augmented_train['combined_text']\n",
        "y = augmented_train['rule_violation']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nValidation set class distribution:\")\n",
        "print(y_val.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating TF-IDF vectorizer...\n",
            "TF-IDF matrix shape - Train: (8116, 10000), Val: (2029, 10000), Test: (10, 10000)\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF Vectorization\n",
        "print(\"Creating TF-IDF vectorizer...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,\n",
        "    ngram_range=(1, 3),\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_df['combined_text'])\n",
        "\n",
        "print(f\"TF-IDF matrix shape - Train: {X_train_tfidf.shape}, Val: {X_val_tfidf.shape}, Test: {X_test_tfidf.shape}\")\n",
        "\n",
        "# Store results for model comparison\n",
        "model_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL 1: LOGISTIC REGRESSION\n",
            "==================================================\n",
            "Logistic Regression Validation AUC: 0.9858\n"
          ]
        }
      ],
      "source": [
        "# Model 1: Logistic Regression on TF-IDF\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "lr_model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred_lr = lr_model.predict_proba(X_val_tfidf)[:, 1]\n",
        "\n",
        "# Calculate AUC\n",
        "lr_auc = roc_auc_score(y_val, y_val_pred_lr)\n",
        "model_results['Logistic Regression'] = lr_auc\n",
        "\n",
        "print(f\"Logistic Regression Validation AUC: {lr_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL 2: RANDOM FOREST\n",
            "==================================================\n",
            "Random Forest Validation AUC: 0.9147\n"
          ]
        }
      ],
      "source": [
        "# Model 2: Random Forest Classifier on TF-IDF\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL 2: RANDOM FOREST\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred_rf = rf_model.predict_proba(X_val_tfidf)[:, 1]\n",
        "\n",
        "# Calculate AUC\n",
        "rf_auc = roc_auc_score(y_val, y_val_pred_rf)\n",
        "model_results['Random Forest'] = rf_auc\n",
        "\n",
        "print(f\"Random Forest Validation AUC: {rf_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL 3: XGBOOST\n",
            "==================================================\n",
            "XGBoost Validation AUC: 0.9479\n"
          ]
        }
      ],
      "source": [
        "# Model 3: XGBoost Classifier on TF-IDF\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL 3: XGBOOST\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=6,\n",
        "    random_state=42,\n",
        "    eval_metric='auc'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred_xgb = xgb_model.predict_proba(X_val_tfidf)[:, 1]\n",
        "\n",
        "# Calculate AUC\n",
        "xgb_auc = roc_auc_score(y_val, y_val_pred_xgb)\n",
        "model_results['XGBoost'] = xgb_auc\n",
        "\n",
        "print(f\"XGBoost Validation AUC: {xgb_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL 4: SVM\n",
            "==================================================\n",
            "SVM Validation AUC: 0.9859\n"
          ]
        }
      ],
      "source": [
        "# Model 4: SVM on TF-IDF\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL 4: SVM\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "svm_model = SVC(\n",
        "    kernel='linear',\n",
        "    probability=True,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred_svm = svm_model.predict_proba(X_val_tfidf)[:, 1]\n",
        "\n",
        "# Calculate AUC\n",
        "svm_auc = roc_auc_score(y_val, y_val_pred_svm)\n",
        "model_results['SVM'] = svm_auc\n",
        "\n",
        "print(f\"SVM Validation AUC: {svm_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL 5: MLP (PyTorch)\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training MLP:  20%|‚ñà‚ñà        | 2/10 [00:46<03:04, 23.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10, Loss: 0.0848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training MLP:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [01:33<02:19, 23.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10, Loss: 0.0279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training MLP:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [02:13<01:26, 21.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10, Loss: 0.0160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training MLP:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [02:52<00:41, 20.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10, Loss: 0.0128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training MLP: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:41<00:00, 22.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, Loss: 0.0119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Validation AUC: 0.9879\n"
          ]
        }
      ],
      "source": [
        "# Model 5: Simple MLP using PyTorch on TF-IDF embeddings\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL 5: MLP (PyTorch)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define MLP model\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1=512, hidden_dim2=256, dropout=0.3):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Convert sparse matrices to dense for PyTorch\n",
        "X_train_dense = X_train_tfidf.toarray()\n",
        "X_val_dense = X_val_tfidf.toarray()\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_dense)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
        "X_val_tensor = torch.FloatTensor(X_val_dense)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train_dense.shape[1]\n",
        "mlp_model = MLPClassifier(input_dim)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "mlp_model.train()\n",
        "for epoch in tqdm(range(10), desc=\"Training MLP\"):\n",
        "    epoch_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp_model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/10, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Predict on validation set\n",
        "mlp_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_val_pred_mlp = mlp_model(X_val_tensor).numpy().flatten()\n",
        "\n",
        "# Calculate AUC\n",
        "mlp_auc = roc_auc_score(y_val, y_val_pred_mlp)\n",
        "model_results['MLP'] = mlp_auc\n",
        "\n",
        "print(f\"MLP Validation AUC: {mlp_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL COMPARISON RESULTS\n",
            "============================================================\n",
            "MLP                 : 0.9879\n",
            "SVM                 : 0.9859\n",
            "Logistic Regression : 0.9858\n",
            "XGBoost             : 0.9479\n",
            "Random Forest       : 0.9147\n",
            "\n",
            "üèÜ BEST MODEL: MLP\n",
            "üèÜ BEST AUC: 0.9879\n"
          ]
        }
      ],
      "source": [
        "# Compare all models and select the best one\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_name, auc_score in sorted(model_results.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{model_name:20s}: {auc_score:.4f}\")\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = max(model_results, key=model_results.get)\n",
        "best_auc = model_results[best_model_name]\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"üèÜ BEST AUC: {best_auc:.4f}\")\n",
        "\n",
        "# Store the best model for predictions\n",
        "if best_model_name == 'Logistic Regression':\n",
        "    best_model = lr_model\n",
        "elif best_model_name == 'Random Forest':\n",
        "    best_model = rf_model\n",
        "elif best_model_name == 'XGBoost':\n",
        "    best_model = xgb_model\n",
        "elif best_model_name == 'SVM':\n",
        "    best_model = svm_model\n",
        "else:  # MLP\n",
        "    best_model = mlp_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "GENERATING PREDICTIONS WITH BEST MODEL: MLP\n",
            "============================================================\n",
            "Training rules: 2\n",
            "Test rules: 2\n",
            "Unseen rules: 0\n",
            "Generated 10 predictions\n",
            "Prediction range: [0.0000, 0.9999]\n",
            "Mean prediction: 0.5505\n"
          ]
        }
      ],
      "source": [
        "# Generate predictions on test set with unseen rule handling\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"GENERATING PREDICTIONS WITH BEST MODEL: {best_model_name}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get unique rules from training and test data\n",
        "train_rules = set(augmented_train['rule'].unique())\n",
        "test_rules = set(test_df['rule'].unique())\n",
        "unseen_rules = test_rules - train_rules\n",
        "\n",
        "print(f\"Training rules: {len(train_rules)}\")\n",
        "print(f\"Test rules: {len(test_rules)}\")\n",
        "print(f\"Unseen rules: {len(unseen_rules)}\")\n",
        "\n",
        "def handle_unseen_rules(test_df, augmented_train, tfidf_vectorizer):\n",
        "    \"\"\"\n",
        "    Handle unseen rules by averaging similarity to positive/negative examples using TF-IDF cosine similarity\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    \n",
        "    for idx, row in test_df.iterrows():\n",
        "        rule = row['rule']\n",
        "        body = row['body']\n",
        "        \n",
        "        if rule in train_rules:\n",
        "            # Use the best model for seen rules\n",
        "            if best_model_name == 'MLP':\n",
        "                X_test_tensor = torch.FloatTensor(X_test_tfidf[idx:idx+1].toarray())\n",
        "                best_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    pred = best_model(X_test_tensor).numpy().flatten()[0]\n",
        "            else:\n",
        "                pred = best_model.predict_proba(X_test_tfidf[idx:idx+1])[0, 1]\n",
        "        else:\n",
        "            # Handle unseen rules using similarity to examples\n",
        "            # Get examples with the same rule\n",
        "            rule_examples = augmented_train[augmented_train['rule'] == rule]\n",
        "            \n",
        "            if len(rule_examples) == 0:\n",
        "                # If no examples found, use global average\n",
        "                pred = 0.5\n",
        "            else:\n",
        "                # Calculate similarity to positive and negative examples\n",
        "                positive_examples = rule_examples[rule_examples['rule_violation'] == 1]['body'].tolist()\n",
        "                negative_examples = rule_examples[rule_examples['rule_violation'] == 0]['body'].tolist()\n",
        "                \n",
        "                if len(positive_examples) > 0 and len(negative_examples) > 0:\n",
        "                    # Vectorize current body and examples\n",
        "                    all_texts = [body] + positive_examples + negative_examples\n",
        "                    similarity_matrix = tfidf_vectorizer.transform(all_texts)\n",
        "                    \n",
        "                    # Calculate cosine similarities\n",
        "                    current_vector = similarity_matrix[0]\n",
        "                    positive_vectors = similarity_matrix[1:1+len(positive_examples)]\n",
        "                    negative_vectors = similarity_matrix[1+len(positive_examples):]\n",
        "                    \n",
        "                    # Calculate average similarities\n",
        "                    pos_similarities = np.array([(current_vector * pos_vec).toarray()[0, 0] / \n",
        "                                             (np.linalg.norm(current_vector.toarray()) * np.linalg.norm(pos_vec.toarray()) + 1e-8)\n",
        "                                             for pos_vec in positive_vectors])\n",
        "                    neg_similarities = np.array([(current_vector * neg_vec).toarray()[0, 0] / \n",
        "                                             (np.linalg.norm(current_vector.toarray()) * np.linalg.norm(neg_vec.toarray()) + 1e-8)\n",
        "                                             for neg_vec in negative_vectors])\n",
        "                    \n",
        "                    avg_pos_sim = np.mean(pos_similarities)\n",
        "                    avg_neg_sim = np.mean(neg_similarities)\n",
        "                    \n",
        "                    # Simple prediction based on similarity ratio\n",
        "                    if avg_pos_sim + avg_neg_sim > 0:\n",
        "                        pred = avg_pos_sim / (avg_pos_sim + avg_neg_sim)\n",
        "                    else:\n",
        "                        pred = 0.5\n",
        "                else:\n",
        "                    pred = 0.5\n",
        "        \n",
        "        predictions.append(pred)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "# Generate predictions\n",
        "test_predictions = handle_unseen_rules(test_df, augmented_train, tfidf_vectorizer)\n",
        "\n",
        "print(f\"Generated {len(test_predictions)} predictions\")\n",
        "print(f\"Prediction range: [{min(test_predictions):.4f}, {max(test_predictions):.4f}]\")\n",
        "print(f\"Mean prediction: {np.mean(test_predictions):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Submission file saved to: submission.csv\n",
            "\n",
            "Submission file shape: (10, 2)\n",
            "\n",
            "First few rows of submission:\n",
            "   row_id  rule_violation\n",
            "0    2029        0.000813\n",
            "1    2030        0.512763\n",
            "2    2031        0.999716\n",
            "3    2032        0.991736\n",
            "4    2033        0.999899\n",
            "\n",
            "Last few rows of submission:\n",
            "   row_id  rule_violation\n",
            "5    2034        0.000034\n",
            "6    2035        0.999705\n",
            "7    2036        0.000002\n",
            "8    2037        0.000085\n",
            "9    2038        0.999858\n"
          ]
        }
      ],
      "source": [
        "# Create submission file\n",
        "submission_df = pd.DataFrame({\n",
        "    'row_id': test_df['row_id'],\n",
        "    'rule_violation': test_predictions\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission_path = 'submission.csv'\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"\\nSubmission file saved to: {submission_path}\")\n",
        "print(f\"\\nSubmission file shape: {submission_df.shape}\")\n",
        "print(f\"\\nFirst few rows of submission:\")\n",
        "print(submission_df.head())\n",
        "print(f\"\\nLast few rows of submission:\")\n",
        "print(submission_df.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "FINAL SUMMARY\n",
            "======================================================================\n",
            "Dataset Information:\n",
            "  - Original training samples: 2029\n",
            "  - Augmented training samples: 10145\n",
            "  - Test samples: 10\n",
            "  - Unique rules in training: 2\n",
            "  - Unique rules in test: 2\n",
            "  - Unseen rules: 0\n",
            "\n",
            "Model Performance:\n",
            "  üèÜ BEST MLP                 : 0.9879\n",
            "     SVM                 : 0.9859\n",
            "     Logistic Regression : 0.9858\n",
            "     XGBoost             : 0.9479\n",
            "     Random Forest       : 0.9147\n",
            "\n",
            "üéØ TARGET ACHIEVED: ‚úÖ YES (Target: ‚â•0.95)\n",
            "üìä BEST MODEL: MLP\n",
            "üìà BEST AUC: 0.9879\n",
            "üíæ SUBMISSION FILE: submission.csv\n",
            "\n",
            "======================================================================\n",
            "NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset Information:\")\n",
        "print(f\"  - Original training samples: {len(train_df)}\")\n",
        "print(f\"  - Augmented training samples: {len(augmented_train)}\")\n",
        "print(f\"  - Test samples: {len(test_df)}\")\n",
        "print(f\"  - Unique rules in training: {len(train_rules)}\")\n",
        "print(f\"  - Unique rules in test: {len(test_rules)}\")\n",
        "print(f\"  - Unseen rules: {len(unseen_rules)}\")\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "for model_name, auc_score in sorted(model_results.items(), key=lambda x: x[1], reverse=True):\n",
        "    status = \"üèÜ BEST\" if model_name == best_model_name else \"  \"\n",
        "    print(f\"  {status} {model_name:20s}: {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\nüéØ TARGET ACHIEVED: {'‚úÖ YES' if best_auc >= 0.95 else '‚ùå NO'} (Target: ‚â•0.95)\")\n",
        "print(f\"üìä BEST MODEL: {best_model_name}\")\n",
        "print(f\"üìà BEST AUC: {best_auc:.4f}\")\n",
        "print(f\"üíæ SUBMISSION FILE: {submission_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
